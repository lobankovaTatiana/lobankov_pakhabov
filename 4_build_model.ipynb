{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Зададим расположение файлов"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "dataset_path = \"D:/Temp/Dataset/kaggle/flickr_30k\" # основной путь к датасету\n",
    "end_dir = dataset_path + \"/copy_flickr30k_images\"\n",
    "\n",
    "path_tokenizer = dataset_path + \"/ru-tokenizer-train.pkl\"\n",
    "path_train_dict = dataset_path + \"/captions-ru-train.pkl\"\n",
    "path_val_dict = dataset_path + \"/captions-ru-val.pkl\"\n",
    "\n",
    "path_features_vgg16 = \"features.pkl\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Загрузка данных"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "def image_names_set(data):\n",
    "    vals = set()\n",
    "\n",
    "    for idx in data.index:\n",
    "        vals.add(data.iat[idx, 0][:-4])\n",
    "\n",
    "    return vals\n",
    "\n",
    "def load_image_features(filename, data):\n",
    "    with open (filename, 'rb') as f:\n",
    "        all_features = pickle.load(f)\n",
    "    features = {k: all_features[k] for k in data}\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Токенизируем данные\n",
    "\n",
    "Подпись будет генерироваться по одному слову за раз. Для того чтобы знать первое и последнее слова\n",
    "мы добавили в предыдущем файле к каждому описанию слова start и end\n",
    "\n",
    "Далее текст подписи кодируется в число (токенизируется)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "def to_lines(data):\n",
    "    all_vals = list()\n",
    "    print(data.keys())\n",
    "    for key in data.keys():\n",
    "        [all_vals.append(d) for d in data[key]]\n",
    "\n",
    "    return all_vals\n",
    "\n",
    "def create_tokenizer(data):\n",
    "    lines = to_lines(data)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def find_max_words(data):\n",
    "    lines = to_lines(data)\n",
    "    return max(len(l.split()) for l in lines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Создание последовательности\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_words, captions_list, image_name):\n",
    "    X_image, X_text, y_word = list(), list(), list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    for caption in captions_list:\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_words)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            X_image.append(image_name)\n",
    "            X_text.append(in_seq)\n",
    "            y_word.append(out_seq)\n",
    "\n",
    "    return X_image, X_text, y_word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Генератор данных\n",
    "Генератор данных будет выдавать данные на одно изображении в каждой партии. Это будут все последовательности, сгенерированные для изображения и её набора описаний.\n",
    "Функция data_generator() будет генератором данных и будет принимать загруженные текстовые описания, признаки изображений, токенизатор и максимальную длину."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def data_generator(tokenizer, max_words, data, images, batch_size, random_seed):\n",
    "    count = 0\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    img_names = list(data.keys())\n",
    "    assert batch_size <= len(img_names), 'batch size must be less than or equal to {}'.format(len(img_names))\n",
    "\n",
    "    while True:\n",
    "        input_img_batch, input_seq_batch, output_word_batch = list(), list(), list()\n",
    "\n",
    "        if count >= len(img_names):\n",
    "            count = 0\n",
    "        start_i = count\n",
    "        end_i = min(len(img_names), count + batch_size)\n",
    "\n",
    "        for i in range(start_i, end_i):\n",
    "            curr_img = img_names[i]\n",
    "            image = images[curr_img][0]\n",
    "            captions_list = data[curr_img]\n",
    "            random.shuffle(captions_list)\n",
    "\n",
    "            input_img, input_seq, output_word = create_sequences(tokenizer, max_words, captions_list, image)\n",
    "\n",
    "            for j in range(len(input_img)):\n",
    "                input_img_batch.append(input_img[j])\n",
    "                input_seq_batch.append(input_seq[j])\n",
    "                output_word_batch.append(output_word[j])\n",
    "\n",
    "        count = count + batch_size\n",
    "        yield [np.array(input_img_batch), np.array(input_seq_batch)], np.array(output_word_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Построение модели"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, TimeDistributed, add\n",
    "\n",
    "\n",
    "def build_rnn(input_size, vocab_size, max_words):\n",
    "    inputs1 = Input(shape=(input_size,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    inputs2 = Input(shape=(max_words,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    de1 = add([fe2, se3])\n",
    "    de2 = Dense(256, activation='relu')(de1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(de2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_alt_rnn(input_size, vocab_size, max_words):\n",
    "    image_input = Input(shape=(input_size,))\n",
    "    fe1 = Dense(256, activation='relu')(image_input)\n",
    "    image_model = RepeatVector(max_words)(fe1)\n",
    "\n",
    "    caption_input = Input(shape=(max_words,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(caption_input)\n",
    "    se2 = LSTM(256, return_sequences=True)(se1)\n",
    "    caption_model = TimeDistributed(Dense(256))(se2)\n",
    "\n",
    "    de1 = add([image_model, caption_model])\n",
    "    de2 = Bidirectional(LSTM(256, return_sequences=False))(de1)\n",
    "    final_model = Dense(vocab_size, activation='softmax')(de2)\n",
    "\n",
    "    model = Model(inputs=[image_input, caption_input], outputs=final_model)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обучение\n",
    "Отбросим загрузку тестового набора данных и контрольные точки модели и просто сохраним модель после каждой эпохи обучения. Затем мы сможем вернуться и загрузить/оценить каждую сохраненную модель после обучения, чтобы найти ту, которая имеет наименьшие потери."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "def load_train_data(train_dict_path, tokenizer_path, features_path):\n",
    "    with open (train_dict_path, 'rb') as f:\n",
    "        out_train_dict = pickle.load(f)\n",
    "    print('кол-во подписей .............. %d' % len(out_train_dict))\n",
    "\n",
    "    out_train_features = load_image_features(features_path, out_train_dict)\n",
    "\n",
    "    with open (tokenizer_path, 'rb') as f:\n",
    "        out_tokenizer = pickle.load(f)\n",
    "    out_vocab_size = len(out_tokenizer.word_index) + 1\n",
    "    print('размер словаря ............... %d' % out_vocab_size)\n",
    "\n",
    "    out_max_words = find_max_words(out_train_dict)\n",
    "    print('длина предложения в словах ... %d' % out_max_words)\n",
    "\n",
    "    return out_train_dict, out_tokenizer, out_vocab_size, out_max_words, out_train_features\n",
    "\n",
    "def train_model(model, train_dict, tokenizer, max_words, train_features, batch_size, epochs_num):\n",
    "    steps = len(train_dict)/batch_size\n",
    "    if len(train_dict) % batch_size != 0:\n",
    "        steps = steps + 1\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs_num):\n",
    "        generator = data_generator(tokenizer, max_words, train_dict, train_features, batch_size, 42)\n",
    "        model.fit(generator,\n",
    "                  epochs=1, steps_per_epoch=steps,\n",
    "                  verbose=1)\n",
    "        model.save('model-' + str(i) + '.h5')\n",
    "\n",
    "    time_difference = time.time() - start_time\n",
    "    minutes = time_difference/60\n",
    "    print('время обучения в минутах ..... %d' % minutes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Набор для обучения"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кол-во подписей .............. 22247\n",
      "размер словаря ............... 38126\n",
      "длина предложения в словах ... 57\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs_num = 20\n",
    "train_dict, tokenizer, vocab_size, max_words, train_features = load_train_data(path_train_dict, path_tokenizer, path_features_vgg16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обучение VGG16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391/1391 [==============================] - 2536s 2s/step - loss: 5.8148\n",
      "1391/1391 [==============================] - 2504s 2s/step - loss: 4.7260\n",
      "1391/1391 [==============================] - 2513s 2s/step - loss: 4.2830\n",
      "1391/1391 [==============================] - 2508s 2s/step - loss: 3.9864\n",
      "1391/1391 [==============================] - 2545s 2s/step - loss: 3.7510\n",
      "1391/1391 [==============================] - 2728s 2s/step - loss: 3.5658\n",
      "1391/1391 [==============================] - 2796s 2s/step - loss: 3.4153\n",
      "1391/1391 [==============================] - 2991s 2s/step - loss: 3.2996\n",
      "1391/1391 [==============================] - 2748s 2s/step - loss: 3.2083\n",
      "1391/1391 [==============================] - 2816s 2s/step - loss: 3.1324\n",
      "1391/1391 [==============================] - 2869s 2s/step - loss: 3.0696\n",
      "1391/1391 [==============================] - 2946s 2s/step - loss: 3.0146\n",
      "1391/1391 [==============================] - 3015s 2s/step - loss: 2.9656\n",
      "1391/1391 [==============================] - 3088s 2s/step - loss: 2.9239\n",
      "1391/1391 [==============================] - 3168s 2s/step - loss: 2.8891\n",
      "1391/1391 [==============================] - 3254s 2s/step - loss: 2.8513\n",
      "1391/1391 [==============================] - 3336s 2s/step - loss: 2.8224\n",
      "1391/1391 [==============================] - 3556s 3s/step - loss: 2.7940\n",
      "1391/1391 [==============================] - 3403s 2s/step - loss: 2.7696\n",
      "1391/1391 [==============================] - 3499s 3s/step - loss: 2.7467\n",
      "время обучения в минутах ..... 981\n"
     ]
    }
   ],
   "source": [
    "model = build_rnn(4096, vocab_size, max_words)\n",
    "train_model(model, train_dict, tokenizer, max_words, train_features, batch_size, epochs_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Обучение модифицированной VGG16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391/1391 [==============================] - 9673s 7s/step - loss: 6.2135\n",
      "1391/1391 [==============================] - 10809s 8s/step - loss: 4.9651\n",
      "1391/1391 [==============================] - 10383s 7s/step - loss: 4.4354\n",
      "1391/1391 [==============================] - 10162s 7s/step - loss: 4.0580\n",
      "1391/1391 [==============================] - 9669s 7s/step - loss: 3.7569\n",
      "1391/1391 [==============================] - 10366s 7s/step - loss: 3.5167\n",
      "1391/1391 [==============================] - 11589s 8s/step - loss: 3.3283\n",
      "1391/1391 [==============================] - 11433s 8s/step - loss: 3.1804\n",
      "1391/1391 [==============================] - 11458s 8s/step - loss: 3.0602\n",
      "1391/1391 [==============================] - 11027s 8s/step - loss: 2.9609\n",
      "1391/1391 [==============================] - 10882s 8s/step - loss: 2.8725\n",
      "1391/1391 [==============================] - 11152s 8s/step - loss: 2.7999\n",
      "1391/1391 [==============================] - 11526s 8s/step - loss: 2.7337\n",
      "1391/1391 [==============================] - 11991s 9s/step - loss: 2.6739\n",
      " 128/1391 [=>............................] - ETA: 3:29:27 - loss: 2.5988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_alt_rnn(4096, vocab_size, max_words)\n",
    "train_model(model, train_dict, tokenizer, max_words, train_features, batch_size, epochs_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}